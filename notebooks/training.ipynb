{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f7dfd9-2be8-4fbe-8300-5ca13b2b7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "\n",
    "import datetime\n",
    "import hashlib\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a32432-6a3a-42cd-b61e-5b7934e3582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, MultiplicativeLR, OneCycleLR\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    GaussianBlur,\n",
    "    Normalize,\n",
    "    RandomGrayscale,\n",
    "    RandomPerspective,\n",
    "    RandomPhotometricDistort,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from doctr import transforms as T\n",
    "from doctr.datasets import VOCABS, RecognitionDataset, WordGenerator\n",
    "from doctr.models import login_to_hub, push_to_hf_hub, recognition\n",
    "from doctr.utils.metrics import TextMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00eac009-5ef8-437e-bf94-ffe875ddd726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_epoch(model, train_loader, batch_transforms, optimizer, scheduler, amp=False):\n",
    "    if amp:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    model.train()\n",
    "    # Iterate over the batches of the dataset\n",
    "    pbar = tqdm(train_loader, position=1)\n",
    "    for images, targets in pbar:\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "        images = batch_transforms(images)\n",
    "\n",
    "        train_loss = model(images, targets)[\"loss\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                train_loss = model(images, targets)[\"loss\"]\n",
    "            scaler.scale(train_loss).backward()\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            # Update the params\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            train_loss = model(images, targets)[\"loss\"]\n",
    "            train_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        pbar.set_description(f\"Training loss: {train_loss.item():.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e3edd5b-7bd3-4df6-8898-03c19738b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, batch_transforms, val_metric, amp=False):\n",
    "    # Model in eval mode\n",
    "    model.eval()\n",
    "    # Reset val metric\n",
    "    val_metric.reset()\n",
    "    # Validation loop\n",
    "    val_loss, batch_cnt = 0, 0\n",
    "    for images, targets in tqdm(val_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "        images = batch_transforms(images)\n",
    "        if amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(images, targets, return_preds=True)\n",
    "        else:\n",
    "            out = model(images, targets, return_preds=True)\n",
    "        # Compute metric\n",
    "        if len(out[\"preds\"]):\n",
    "            words, _ = zip(*out[\"preds\"])\n",
    "        else:\n",
    "            words = []\n",
    "        val_metric.update(targets, words)\n",
    "\n",
    "        val_loss += out[\"loss\"].item()\n",
    "        batch_cnt += 1\n",
    "\n",
    "    val_loss /= batch_cnt\n",
    "    result = val_metric.summary()\n",
    "    return val_loss, result[\"raw\"], result[\"unicase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38989aea-6020-48a7-bb14-ab77aca62ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience: int = 5, min_delta: float = 0.01):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "\n",
    "    def early_stop(self, validation_loss: float) -> bool:\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ffc1be8-f56a-402b-be2a-1cd47cf2ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(args)\n",
    "\n",
    "    if not isinstance(args.workers, int):\n",
    "        args.workers = min(16, mp.cpu_count())\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    vocab = VOCABS[args.vocab]\n",
    "    fonts = args.font.split(\",\")\n",
    "\n",
    "    # Load val data generator\n",
    "    st = time.time()\n",
    "    if isinstance(args.val_path, str):\n",
    "        with open(os.path.join(args.val_path, \"labels.json\"), \"rb\") as f:\n",
    "            val_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "        val_set = RecognitionDataset(\n",
    "            img_folder=os.path.join(args.val_path, \"images\"),\n",
    "            labels_path=os.path.join(args.val_path, \"labels.json\"),\n",
    "            img_transforms=T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),\n",
    "        )\n",
    "    else:\n",
    "        val_hash = None\n",
    "        # Load synthetic data generator\n",
    "        val_set = WordGenerator(\n",
    "            vocab=vocab,\n",
    "            min_chars=args.min_chars,\n",
    "            max_chars=args.max_chars,\n",
    "            num_samples=args.val_samples * len(vocab),\n",
    "            font_family=fonts,\n",
    "            img_transforms=Compose([\n",
    "                T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),\n",
    "                # Ensure we have a 90% split of white-background images\n",
    "                T.RandomApply(T.ColorInversion(), 0.9),\n",
    "            ]),\n",
    "        )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=False,\n",
    "        num_workers=args.workers,\n",
    "        sampler=SequentialSampler(val_set),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=val_set.collate_fn,\n",
    "    )\n",
    "    print(f\"Validation set loaded in {time.time() - st:.4}s ({len(val_set)} samples in \" f\"{len(val_loader)} batches)\")\n",
    "\n",
    "    batch_transforms = Normalize(mean=(0.694, 0.695, 0.693), std=(0.299, 0.296, 0.301))\n",
    "\n",
    "    # Load doctr model\n",
    "    model = recognition.__dict__[args.arch](pretrained=args.pretrained, vocab=vocab)\n",
    "\n",
    "    # Resume weights\n",
    "    if isinstance(args.resume, str):\n",
    "        print(f\"Resuming {args.resume}\")\n",
    "        checkpoint = torch.load(args.resume, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint)\n",
    "\n",
    "    # GPU\n",
    "    if isinstance(args.device, int):\n",
    "        if not torch.cuda.is_available():\n",
    "            raise AssertionError(\"PyTorch cannot access your GPU. Please investigate!\")\n",
    "        if args.device >= torch.cuda.device_count():\n",
    "            raise ValueError(\"Invalid device index\")\n",
    "    # Silent default switch to GPU if available\n",
    "    elif torch.cuda.is_available():\n",
    "        args.device = 0\n",
    "    else:\n",
    "        logging.warning(\"No accessible GPU, targe device set to CPU.\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_device(args.device)\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Metrics\n",
    "    val_metric = TextMatch()\n",
    "\n",
    "    if args.test_only:\n",
    "        print(\"Running evaluation\")\n",
    "        val_loss, exact_match, partial_match = evaluate(model, val_loader, batch_transforms, val_metric, amp=args.amp)\n",
    "        print(f\"Validation loss: {val_loss:.6} (Exact: {exact_match:.2%} | Partial: {partial_match:.2%})\")\n",
    "        return\n",
    "\n",
    "    st = time.time()\n",
    "\n",
    "    if isinstance(args.train_path, str):\n",
    "        # Load train data generator\n",
    "        base_path = Path(args.train_path)\n",
    "        parts = (\n",
    "            [base_path]\n",
    "            if base_path.joinpath(\"labels.json\").is_file()\n",
    "            else [base_path.joinpath(sub) for sub in os.listdir(base_path)]\n",
    "        )\n",
    "        with open(parts[0].joinpath(\"labels.json\"), \"rb\") as f:\n",
    "            train_hash = hashlib.sha256(f.read()).hexdigest()\n",
    "\n",
    "        train_set = RecognitionDataset(\n",
    "            parts[0].joinpath(\"images\"),\n",
    "            parts[0].joinpath(\"labels.json\"),\n",
    "            img_transforms=Compose([\n",
    "                T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),\n",
    "                # Augmentations\n",
    "                T.RandomApply(T.ColorInversion(), 0.1),\n",
    "                RandomGrayscale(p=0.1),\n",
    "                RandomPhotometricDistort(p=0.1),\n",
    "                T.RandomApply(T.RandomShadow(), p=0.4),\n",
    "                T.RandomApply(T.GaussianNoise(mean=0, std=0.1), 0.1),\n",
    "                T.RandomApply(GaussianBlur(3), 0.3),\n",
    "                RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            ]),\n",
    "        )\n",
    "        if len(parts) > 1:\n",
    "            for subfolder in parts[1:]:\n",
    "                train_set.merge_dataset(\n",
    "                    RecognitionDataset(subfolder.joinpath(\"images\"), subfolder.joinpath(\"labels.json\"))\n",
    "                )\n",
    "    else:\n",
    "        train_hash = None\n",
    "        # Load synthetic data generator\n",
    "        train_set = WordGenerator(\n",
    "            vocab=vocab,\n",
    "            min_chars=args.min_chars,\n",
    "            max_chars=args.max_chars,\n",
    "            num_samples=args.train_samples * len(vocab),\n",
    "            font_family=fonts,\n",
    "            img_transforms=Compose([\n",
    "                T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),\n",
    "                # Ensure we have a 90% split of white-background images\n",
    "                T.RandomApply(T.ColorInversion(), 0.9),\n",
    "                RandomGrayscale(p=0.1),\n",
    "                RandomPhotometricDistort(p=0.1),\n",
    "                T.RandomApply(T.RandomShadow(), p=0.4),\n",
    "                T.RandomApply(T.GaussianNoise(mean=0, std=0.1), 0.1),\n",
    "                T.RandomApply(GaussianBlur(3), 0.3),\n",
    "                RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            ]),\n",
    "        )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        drop_last=True,\n",
    "        num_workers=args.workers,\n",
    "        sampler=RandomSampler(train_set),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=train_set.collate_fn,\n",
    "    )\n",
    "    print(f\"Train set loaded in {time.time() - st:.4}s ({len(train_set)} samples in \" f\"{len(train_loader)} batches)\")\n",
    "\n",
    "    if args.show_samples:\n",
    "        x, target = next(iter(train_loader))\n",
    "        plot_samples(x, target)\n",
    "        return\n",
    "\n",
    "    # Backbone freezing\n",
    "    if args.freeze_backbone:\n",
    "        for p in model.feat_extractor.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        args.lr,\n",
    "        betas=(0.95, 0.99),\n",
    "        eps=1e-6,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    # LR Finder\n",
    "    # if args.find_lr:\n",
    "    #     lrs, losses = record_lr(model, train_loader, batch_transforms, optimizer, amp=args.amp)\n",
    "    #     plot_recorder(lrs, losses)\n",
    "    #     return\n",
    "    # Scheduler\n",
    "    if args.sched == \"cosine\":\n",
    "        scheduler = CosineAnnealingLR(optimizer, args.epochs * len(train_loader), eta_min=args.lr / 25e4)\n",
    "    elif args.sched == \"onecycle\":\n",
    "        scheduler = OneCycleLR(optimizer, args.lr, args.epochs * len(train_loader))\n",
    "\n",
    "    # Training monitoring\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    exp_name = f\"{args.arch}_{current_time}\" if args.name is None else args.name\n",
    "\n",
    "    # Create loss queue\n",
    "    min_loss = np.inf\n",
    "    # Training loop\n",
    "    if args.early_stop:\n",
    "        early_stopper = EarlyStopper(patience=args.early_stop_epochs, min_delta=args.early_stop_delta)\n",
    "    for epoch in range(args.epochs):\n",
    "        fit_one_epoch(model, train_loader, batch_transforms, optimizer, scheduler, amp=args.amp)\n",
    "\n",
    "        # Validation loop at the end of each epoch\n",
    "        val_loss, exact_match, partial_match = evaluate(model, val_loader, batch_transforms, val_metric, amp=args.amp)\n",
    "        if val_loss < min_loss:\n",
    "            print(f\"Validation loss decreased {min_loss:.6} --> {val_loss:.6}: saving state...\")\n",
    "            torch.save(model.state_dict(), f\"./{exp_name}.pt\")\n",
    "            min_loss = val_loss\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{args.epochs} - Validation loss: {val_loss:.6} \"\n",
    "            f\"(Exact: {exact_match:.2%} | Partial: {partial_match:.2%})\"\n",
    "        )\n",
    "        if args.early_stop and early_stopper.early_stop(val_loss):\n",
    "            print(\"Training halted early due to reaching patience limit.\")\n",
    "            break\n",
    "    if args.wb:\n",
    "        run.finish()\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        push_to_hf_hub(model, exp_name, task=\"recognition\", run_config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d021cd5-89a6-4e8d-a643-7eb09c2e1bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"DocTR training script for text recognition (PyTorch)\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"arch\", type=str, help=\"text-recognition model to train\")\n",
    "    parser.add_argument(\"--train_path\", type=str, default=None, help=\"path to train data folder(s)\")\n",
    "    parser.add_argument(\"--val_path\", type=str, default=None, help=\"path to val data folder\")\n",
    "    parser.add_argument(\n",
    "        \"--train-samples\",\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help=\"Multiplied by the vocab length gets you the number of synthetic training samples that will be used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--val-samples\",\n",
    "        type=int,\n",
    "        default=20,\n",
    "        help=\"Multiplied by the vocab length gets you the number of synthetic validation samples that will be used.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--font\", type=str, default=\"FreeMono.ttf,FreeSans.ttf,FreeSerif.ttf\", help=\"Font family to be used\"\n",
    "    )\n",
    "    parser.add_argument(\"--min-chars\", type=int, default=1, help=\"Minimum number of characters per synthetic sample\")\n",
    "    parser.add_argument(\"--max-chars\", type=int, default=12, help=\"Maximum number of characters per synthetic sample\")\n",
    "    parser.add_argument(\"--name\", type=str, default=None, help=\"Name of your training experiment\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10, help=\"number of epochs to train the model on\")\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=64, help=\"batch size for training\")\n",
    "    parser.add_argument(\"--device\", default=None, type=int, help=\"device\")\n",
    "    parser.add_argument(\"--input_size\", type=int, default=32, help=\"input size H for the model, W = 4*H\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate for the optimizer (Adam)\")\n",
    "    parser.add_argument(\"--wd\", \"--weight-decay\", default=0, type=float, help=\"weight decay\", dest=\"weight_decay\")\n",
    "    parser.add_argument(\"-j\", \"--workers\", type=int, default=None, help=\"number of workers used for dataloading\")\n",
    "    parser.add_argument(\"--resume\", type=str, default=None, help=\"Path to your checkpoint\")\n",
    "    parser.add_argument(\"--vocab\", type=str, default=\"french\", help=\"Vocab to be used for training\")\n",
    "    parser.add_argument(\"--test-only\", dest=\"test_only\", action=\"store_true\", help=\"Run the validation loop\")\n",
    "    parser.add_argument(\n",
    "        \"--freeze-backbone\", dest=\"freeze_backbone\", action=\"store_true\", help=\"freeze model backbone for fine-tuning\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--show-samples\", dest=\"show_samples\", action=\"store_true\", help=\"Display unormalized training samples\"\n",
    "    )\n",
    "    parser.add_argument(\"--wb\", dest=\"wb\", action=\"store_true\", help=\"Log to Weights & Biases\")\n",
    "    parser.add_argument(\"--push-to-hub\", dest=\"push_to_hub\", action=\"store_true\", help=\"Push to Huggingface Hub\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained\",\n",
    "        dest=\"pretrained\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Load pretrained parameters before starting the training\",\n",
    "    )\n",
    "    parser.add_argument(\"--sched\", type=str, default=\"cosine\", help=\"scheduler to use\")\n",
    "    parser.add_argument(\"--amp\", dest=\"amp\", help=\"Use Automatic Mixed Precision\", action=\"store_true\")\n",
    "    parser.add_argument(\"--find-lr\", action=\"store_true\", help=\"Gridsearch the optimal LR\")\n",
    "    parser.add_argument(\"--early-stop\", action=\"store_true\", help=\"Enable early stopping\")\n",
    "    parser.add_argument(\"--early-stop-epochs\", type=int, default=5, help=\"Patience for early stopping\")\n",
    "    parser.add_argument(\"--early-stop-delta\", type=float, default=0.01, help=\"Minimum Delta for early stopping\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4558699e-26b2-45b3-bcbe-2ca3c939783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = get_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4ba6b03-2554-4a96-888b-32122cf56bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['sar_resnet31', \n",
    "            '--train_path', '../data/content/datasets/train/',\n",
    "            '--val_path', '../data/content/datasets/val/',\n",
    "            '-b', '4',\n",
    "            '--input_size', '32',\n",
    "            '--max-chars', '42',\n",
    "            '--pretrained']\n",
    "            # '--show-samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2209dd4c-6bfa-4754-a13c-d19937bcc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb46b618-ce57-4381-8a37-9e3cb90d80e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(amp=False, arch='sar_resnet31', batch_size=4, device=None, early_stop=False, early_stop_delta=0.01, early_stop_epochs=5, epochs=10, find_lr=False, font='FreeMono.ttf,FreeSans.ttf,FreeSerif.ttf', freeze_backbone=False, input_size=32, lr=0.001, max_chars=42, min_chars=1, name=None, pretrained=True, push_to_hub=False, resume=None, sched='cosine', show_samples=False, test_only=False, train_path='../data/content/datasets/train/', train_samples=1000, val_path='../data/content/datasets/val/', val_samples=20, vocab='french', wb=False, weight_decay=0, workers=None)\n",
      "Validation set loaded in 0.001112s (20 samples in 5 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Invalid model URL, using default initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set loaded in 0.000659s (20 samples in 5 batches)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Training loss: 4.69709:   0%|                                                                                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Training loss: 4.69709:  20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  2.12it/s]\u001b[A\n",
      "Training loss: 4.51038:  20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  2.12it/s]\u001b[A\n",
      "Training loss: 4.51038:  40%|██████████████████████████████████████▊                                                          | 2/5 [00:00<00:00,  3.38it/s]\u001b[A\n",
      "Training loss: 3.59564:  40%|██████████████████████████████████████▊                                                          | 2/5 [00:00<00:00,  3.38it/s]\u001b[A\n",
      "Training loss: 3.59564:  60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  4.52it/s]\u001b[A\n",
      "Training loss: 3.26175:  60%|██████████████████████████████████████████████████████████▏                                      | 3/5 [00:00<00:00,  4.52it/s]\u001b[A\n",
      "Training loss: 3.26175:  80%|█████████████████████████████████████████████████████████████████████████████▌                   | 4/5 [00:00<00:00,  5.38it/s]\u001b[A\n",
      "Training loss: 3.10335:  80%|█████████████████████████████████████████████████████████████████████████████▌                   | 4/5 [00:01<00:00,  5.38it/s]\u001b[A\n",
      "Training loss: 3.10335: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  4.70it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased inf --> 126.916: saving state...\n",
      "Epoch 1/10 - Validation loss: 126.916 (Exact: 0.00% | Partial: 0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                 | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Training loss: 3.06687:   0%|                                                                                                         | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Training loss: 3.06687:  20%|███████████████████▍                                                                             | 1/5 [00:00<00:01,  2.39it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 190\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    188\u001b[0m     early_stopper \u001b[38;5;241m=\u001b[39m EarlyStopper(patience\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mearly_stop_epochs, min_delta\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mearly_stop_delta)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 190\u001b[0m     \u001b[43mfit_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Validation loop at the end of each epoch\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     val_loss, exact_match, partial_match \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, batch_transforms, val_metric, amp\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mamp)\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mfit_one_epoch\u001b[0;34m(model, train_loader, batch_transforms, optimizer, scheduler, amp)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m model(images, targets)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/workspace/chequeparser/venv/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/chequeparser/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "161ccfc4-0da9-404f-addb-44f81fff3cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_tf_available: False\n",
      "is_torch_available: True\n"
     ]
    }
   ],
   "source": [
    "from doctr.file_utils import is_tf_available, is_torch_available\n",
    "\n",
    "print(f\"is_tf_available: {is_tf_available()}\")\n",
    "print(f\"is_torch_available: {is_torch_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e6e03-ff47-4693-8610-c4c83e6a911a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
